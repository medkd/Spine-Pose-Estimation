{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb5d49-d7ed-46db-90c1-61f234d8057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import pandas as pd\n",
    "from vertebra_4_point import *\n",
    "from vertebra_centrioid_model import *\n",
    "\n",
    "heat_img_height = 384\n",
    "heat_img_width = 224\n",
    "input_height = 768\n",
    "input_width = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2776b3-8a46-485b-9fdc-aed76fe20b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model_4 = StackedHourglassNetwork_4(input_shape=(768,448,3), num_stack=4, num_residual=1,num_heatmap=69,num_seg = 17)\n",
    "pose_model_4.load_weights('./test_with_seg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cee3b-8796-4fe6-ac7c-ac73442bb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = StackedHourglassNetwork(input_shape=(768,448,3), num_stack=4, num_residual=1,num_heatmap=17,num_seg = 17)\n",
    "pose_model.load_weights('./test_with_with_centrioid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28871eb-deb5-4764-a323-7fc8d7329044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for centrioid model\n",
    "def find_max_coordinates(heatmaps):\n",
    "    \n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (heat_img_height*heat_img_width, 17))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    # after flatten, each 64 values represent one row in original heatmap\n",
    "    y = tf.cast(indices / heat_img_width, dtype=tf.int64)\n",
    "    x = indices - heat_img_width * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "# for centrioid model\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "    # pad the heatmap so that we don't need to deal with borders\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]])\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        # since we've padded the heatmap, the max keypoint should increment by 1\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        # the patch is the 3x3 grid around the max keypoint location\n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        # assign 0 to max location\n",
    "        patch[1][1] = 0\n",
    "        # and the next largest value is the largest neigbour we are looking for\n",
    "        index = np.argmax(patch)\n",
    "        # find out the location of it relative to center\n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        # we can then add original max keypoint location with this offset\n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "    # we do need to clip the value to make sure there's no keypoint out of border, just in case.\n",
    "    \n",
    "    normalized_keypoints = []\n",
    "    for j in adjusted_keypoints:\n",
    "        norm_x = np.clip(j[0],0,heat_img_width) / heat_img_width\n",
    "        norm_y = np.clip(j[1],0,heat_img_height) / heat_img_height\n",
    "        normalized_keypoints.append((norm_x, norm_y))\n",
    "    \n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558173b8-2e71-4414-b3ac-d81dac801e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 4 point model\n",
    "def find_max_coordinates_4(heatmaps):\n",
    "    \n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (heat_img_height*heat_img_width, 69))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    # after flatten, each 64 values represent one row in original heatmap\n",
    "    y = tf.cast(indices / heat_img_width, dtype=tf.int64)\n",
    "    x = indices - heat_img_width * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "# for 4 point model\n",
    "def extract_keypoints_from_heatmap_4(heatmaps):\n",
    "    max_keypoints = find_max_coordinates_4(heatmaps)\n",
    "    # pad the heatmap so that we don't need to deal with borders\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]])\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        # since we've padded the heatmap, the max keypoint should increment by 1\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        # the patch is the 3x3 grid around the max keypoint location\n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        # assign 0 to max location\n",
    "        patch[1][1] = 0\n",
    "        # and the next largest value is the largest neigbour we are looking for\n",
    "        index = np.argmax(patch)\n",
    "        # find out the location of it relative to center\n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        # we can then add original max keypoint location with this offset\n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "    # we do need to clip the value to make sure there's no keypoint out of border, just in case.\n",
    "    \n",
    "    normalized_keypoints = []\n",
    "    for j in adjusted_keypoints:\n",
    "        norm_x = np.clip(j[0],0,heat_img_width) / heat_img_width\n",
    "        norm_y = np.clip(j[1],0,heat_img_height) / heat_img_height\n",
    "        normalized_keypoints.append((norm_x, norm_y))\n",
    "    \n",
    "    return normalized_keypoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdc2e7-d1c6-4d33-8344-24a0d00eac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, true_keypoints, dcm_ID,index=None):\n",
    "    fig = plt.figure(figsize = (12,7))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "   \n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=1, c='red', marker='o')\n",
    "    \n",
    "    \n",
    "    for i, joint in enumerate(true_keypoints):\n",
    "        joint_x = joint[0]\n",
    "        joint_y = joint[1]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=1, c='yellow', marker='o')\n",
    "    \n",
    "    \n",
    "    #plt.savefig(os.path.join('/home/u8227385/val_spine_graph', dcm_ID+'.jpg'))\n",
    "    plt.show()\n",
    "\n",
    "def draw_true_on_image(image, true_keypoints, dcm_ID,index=None):\n",
    "    fig = plt.figure(figsize = (12,7))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "   \n",
    "    \n",
    "    \n",
    "    for i, joint in enumerate(true_keypoints):\n",
    "        joint_x = joint[0]\n",
    "        joint_y = joint[1]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=1, c='yellow', marker='o')\n",
    "    \n",
    "    \n",
    "    plt.savefig(os.path.join('/Users/linweichen/Desktop/NTU_ortho/Spine/spine_examination', dcm_ID+'.jpg'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c58e2a-308b-4a6d-82df-c543ae0642aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(line1, line2):\n",
    "    vector1 = (line1[1][0] - line1[0][0], line1[1][1] - line1[0][1])\n",
    "    vector2 = (line2[1][0] - line2[0][0], line2[1][1] - line2[0][1])\n",
    "    \n",
    "    dot_product = vector1[0] * vector2[0] + vector1[1] * vector2[1]\n",
    "    magnitude1 = math.sqrt(vector1[0]**2 + vector1[1]**2)\n",
    "    magnitude2 = math.sqrt(vector2[0]**2 + vector2[1]**2)\n",
    "    \n",
    "    cos_angle = dot_product / (magnitude1 * magnitude2)\n",
    "    angle = math.degrees(math.acos(cos_angle))\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e24bd-cf6a-4911-a21f-fe116caebc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "import math\n",
    "\n",
    "def polynomial_curve_fit(x, y, degree):\n",
    "    coefficients = np.polyfit(x, y, degree)\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "    return polynomial\n",
    "\n",
    "def calculate_area_difference(x, y, poly_func):\n",
    "    segment_linear_areas = []\n",
    "    segment_poly_areas = []\n",
    "\n",
    "    for i in range(len(x) - 1):\n",
    "        # Linear area (trapezoid)\n",
    "        segment_linear_area = 0.5 * (y[i] + y[i + 1]) * (x[i + 1] - x[i])\n",
    "        segment_linear_area = abs(segment_linear_area)\n",
    "        segment_linear_areas.append(segment_linear_area)\n",
    "\n",
    "        # Polynomial area (integration)\n",
    "        segment_poly_area, _ = quad(poly_func, x[i], x[i + 1])\n",
    "        segment_poly_areas.append(abs(segment_poly_area))\n",
    "\n",
    "    # Calculate total area differences\n",
    "    total_difference = sum(abs(lp - pp) for lp, pp in zip(segment_linear_areas, segment_poly_areas))\n",
    "    return total_difference\n",
    "\n",
    "def find_best_polynomial_degree(x, y, max_degree=7):\n",
    "    best_degree = None\n",
    "    min_area_difference = float('inf')\n",
    "    r_squared_values = []\n",
    "\n",
    "    for degree in range(max_degree + 1):\n",
    "        # Fit polynomial curve\n",
    "        poly_func = polynomial_curve_fit(x, y, degree)\n",
    "\n",
    "        # Calculate area difference\n",
    "        area_difference = calculate_area_difference(x, y, poly_func)\n",
    "\n",
    "        # Calculate R^2\n",
    "        y_mean = np.mean(y)\n",
    "        ss_total = sum((yi - y_mean)**2 for yi in y)\n",
    "        ss_residual = sum((yi - poly_func(xi))**2 for xi, yi in zip(x, y))\n",
    "        r_squared = 1 - (ss_residual / ss_total)\n",
    "        r_squared_values.append((degree, r_squared))\n",
    "\n",
    "        # Update the best degree based on area difference\n",
    "        if area_difference < min_area_difference:\n",
    "            min_area_difference = area_difference\n",
    "            best_degree = degree\n",
    "\n",
    "    return best_degree, r_squared_values\n",
    "\n",
    "def find_apex_and_limits(points, poly_func):\n",
    "    first_derivative = poly_func.deriv(m=1)\n",
    "    second_derivative = poly_func.deriv(m=2)\n",
    "    \n",
    "    # Find roots of the first derivative (apex candidates)\n",
    "    critical_points = np.roots(first_derivative)\n",
    "    critical_points = critical_points[np.isreal(critical_points)].real  # Keep only real roots\n",
    "    \n",
    "    apexes = {}\n",
    "    for cp in critical_points:\n",
    "        closest_key = min(points.keys(), key=lambda k: abs(points[k][0] - cp))\n",
    "        if closest_key not in ['t1', 'l5']:  # Skip if apex is 't1' or 'l5'\n",
    "            apexes[closest_key] = cp\n",
    "    \n",
    "    # Find roots of the second derivative (inflection points)\n",
    "    inflection_points = np.roots(second_derivative)\n",
    "    inflection_points = inflection_points[np.isreal(inflection_points)].real\n",
    "    \n",
    "    # Determine upper and lower limits\n",
    "    results = {}\n",
    "    for apex_key, apex_x in apexes.items():\n",
    "        left_inflection = max([ip for ip in inflection_points if ip < apex_x], default=None)\n",
    "        right_inflection = min([ip for ip in inflection_points if ip > apex_x], default=None)\n",
    "        \n",
    "        upper_limit = min(points.keys(), key=lambda k: abs(points[k][0] - left_inflection)) if left_inflection else 't1'\n",
    "        lower_limit = min(points.keys(), key=lambda k: abs(points[k][0] - right_inflection)) if right_inflection else 'l5'\n",
    "        \n",
    "        # Skip cases where apex overlaps with limits\n",
    "        if upper_limit == apex_key:\n",
    "            upper_limit = None\n",
    "        if lower_limit == apex_key:\n",
    "            lower_limit = None\n",
    "        \n",
    "        if upper_limit is not None and lower_limit is not None:\n",
    "            results[apex_key] = {\n",
    "                \"apex_x\": apex_x,\n",
    "                \"upper_limit\": upper_limit,\n",
    "                \"lower_limit\": lower_limit\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_apex_and_limits(results):\n",
    "    for apex_key, data in results.items():\n",
    "        # Skip output if apex is the same as upper or lower limit\n",
    "        if data['upper_limit'] == apex_key or data['lower_limit'] == apex_key:\n",
    "            continue\n",
    "\n",
    "        #print(f\"Apex: {apex_key} at x = {data['apex_x']}\")\n",
    "        print(f\"Apex: {apex_key}\")\n",
    "        print(f\"Upper Endplate: {data['upper_limit']}\")\n",
    "        print(f\"Lower Endplate: {data['lower_limit']}\")\n",
    "        \n",
    "        if data['angle'] is not None:\n",
    "            print(f\"Cobb angle: {data['angle']} degrees\")\n",
    "        print('')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def comparing_true_and_pred_ntuh(dcm_ID):\n",
    "\n",
    "    img = pydicom.dcmread(dcm_ID + '.dcm', force=True).pixel_array \n",
    "    df = pd.read_csv(dcm_ID + '_new_with_center.csv', sep = ',')\n",
    "\n",
    "    lst = list(df.columns)\n",
    "    true_lst = []\n",
    "    # from 69 to last\n",
    "    for k in range(69,86,1):\n",
    "        x = df[lst[2*k]][0]\n",
    "        y = df[lst[2*k+1]][0]\n",
    "        true_lst.append((x,y))\n",
    "    height, width = img.shape\n",
    "    \n",
    "    dcm = pydicom.dcmread(dcm_ID + '.dcm', force=True)\n",
    "\n",
    "    dcm_img = dcm.pixel_array\n",
    "\n",
    "    height, width = dcm_img.shape\n",
    "    scale_x = heat_img_width/width\n",
    "    scale_y = heat_img_height/height   \n",
    "    resize_params = dcm.get(0x00280030).value\n",
    "    abs_lst = []\n",
    "\n",
    "    ##\n",
    "    new_img = 255 - ((dcm_img - dcm_img.min()) / (dcm_img.max() - dcm_img.min())*255).astype(np.uint8)\n",
    "    raw_img = cv2.resize(new_img, (448,768),interpolation=cv2.INTER_CUBIC)\n",
    "    color_img = cv2.cvtColor(raw_img,  cv2.COLOR_GRAY2BGR)/255\n",
    "\n",
    "    inputs_neg = tf.expand_dims(color_img, 0)\n",
    "\n",
    "\n",
    "    outputs = pose_model([inputs_neg], training=False)\n",
    "        ##\n",
    "\n",
    "    heatmap = tf.squeeze(outputs[3], axis=0).numpy()\n",
    "\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "\n",
    "    joints = []\n",
    "    for i, joint in enumerate(kp):\n",
    "        joint_x = joint[0] * width\n",
    "        joint_y = joint[1] * height\n",
    "        joints.append((joint_x,joint_y))\n",
    "\n",
    "\n",
    "    points = {'t1':[],'t2':[],'t3':[],'t4':[],'t5':[],'t6':[],'t7':[],'t8':[],'t9':[],'t10':[],'t11':[],'t12':[],'l1':[],'l2':[],'l3':[],'l4':[],'l5':[]}\n",
    "    count = 0\n",
    "    for i in points:\n",
    "        points[i].append(true_lst[count][1])\n",
    "        points[i].append(true_lst[count][0])\n",
    "\n",
    "        count+=1\n",
    "\n",
    "    x = np.array([v[0] for v in points.values()])\n",
    "    y = np.array([v[1] for v in points.values()])\n",
    "    \n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(x,y)\n",
    "    plt.xlim(0,height)\n",
    "    plt.ylim(0,width)\n",
    "\n",
    "    # Find the best polynomial degree\n",
    "    best_degree, r_squared_values = find_best_polynomial_degree(x, y, max_degree=7)\n",
    "    print('Predicted Result')\n",
    "    print(f\"Best polynomial degree: {best_degree}\")\n",
    "    # Fit polynomial curve\n",
    "    \n",
    "    poly_func = polynomial_curve_fit(x, y, best_degree)\n",
    "\n",
    "    # Plot the first derivative\n",
    "    first_derivative = poly_func.deriv(m=1)\n",
    "    x_vals = np.linspace(min(x), max(x), 500)\n",
    "    y_vals = first_derivative(x_vals)\n",
    "\n",
    "    results = find_apex_and_limits(points, poly_func)\n",
    "   \n",
    "    draw_true_on_image(dcm_img, true_lst, dcm_ID,index=None)\n",
    "    draw_keypoints_on_image(dcm_img, kp, true_lst, dcm_ID,index=None)\n",
    "    \n",
    "    ## for 4 point and then calculate angle\n",
    "    outputs_4 = pose_model_4([inputs_neg], training=False)\n",
    "\n",
    "\n",
    "    heatmap_4 = tf.squeeze(outputs_4[3], axis=0).numpy()\n",
    "\n",
    "    kp_4 = extract_keypoints_from_heatmap_4(heatmap_4)\n",
    "\n",
    "    joints = []\n",
    "    for i, joint in enumerate(kp_4):\n",
    "        joint_x = joint[0] * width\n",
    "        joint_y = joint[1] * height\n",
    "        joints.append((joint_x,joint_y))\n",
    "\n",
    "\n",
    "    new_xy = {'t1':[],'t2':[],'t3':[],'t4':[],'t5':[],'t6':[],'t7':[],'t8':[],'t9':[],'t10':[],'t11':[],'t12':[],'l1':[],'l2':[],'l3':[],'l4':[],'l5':[]}\n",
    "    count = 0\n",
    "    for i in new_xy:\n",
    "        new_xy[i].append((joints[4*count][0], joints[4*count][1]))\n",
    "        new_xy[i].append((joints[4*count+1][0], joints[4*count+1][1]))\n",
    "        new_xy[i].append((joints[4*count+2][0], joints[4*count+2][1]))\n",
    "        new_xy[i].append((joints[4*count+3][0], joints[4*count+3][1]))\n",
    "        count+=1    \n",
    "    \n",
    "    for result in results:\n",
    "        \n",
    "        upper_limit_coords = None\n",
    "        upper_limit_coords = new_xy[results[result][\"upper_limit\"]][:2]\n",
    "\n",
    "\n",
    "        lower_limit_coords = None\n",
    "        lower_limit_coords = new_xy[results[result][\"lower_limit\"]][2:]\n",
    "\n",
    "        # Calculate the angle between the lines if limits are found\n",
    "        angle = None\n",
    "        if upper_limit_coords and lower_limit_coords:\n",
    "            angle = calculate_angle(upper_limit_coords, lower_limit_coords)\n",
    "\n",
    "        results[result]['angle'] = angle\n",
    "    \n",
    "    \n",
    "    \n",
    "    print_apex_and_limits(results)\n",
    "    \n",
    "    print('....')\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44110c-ae42-4d34-bb91-93a513eeeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "os.chdir('folder to your dcm file')\n",
    "ID = \"\"\n",
    "results = comparing_true_and_pred_ntuh(ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
